{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "SFiQswuE87ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **T5_BASE F1_Score**"
      ],
      "metadata": {
        "id": "m3ZsKuyaS98q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# set up the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# load the pretrained model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# define the function to generate answers\n",
        "def generate_answer(question, context):\n",
        "    input_str = \"question: \" + question + \" context: \" + context #+ \" </s>\"\n",
        "    input_ids = tokenizer.encode(input_str, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids)\n",
        "    # print(\"Output\")\n",
        "    # print(tokenizer.decode(outputs[0]))\n",
        "    tem = tokenizer.decode(outputs[0])\n",
        "    ans = \"\"\n",
        "    n = len(tem)\n",
        "    f = True\n",
        "    for c in range(0,n):\n",
        "      if tem[c]=='<':\n",
        "        f = False\n",
        "      elif tem[c]=='>':\n",
        "        f = True\n",
        "      else:\n",
        "        if f==True:\n",
        "          ans = ans + tem[c]\n",
        "    print(ans)\n",
        "    return ans\n",
        "    # return tokenizer.decode(outputs[0])\n",
        "\n",
        "# define the function to calculate F1 score\n",
        "def calculate_f1_score(true_labels, predicted_labels):\n",
        "    return f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# example sentences\n",
        "sentences = [\n",
        "  \"Mako_Shark and Great_White_Shark are from same hypothetical_ancestor_ 1 .\",\n",
        "  \"Angel_Shark and hypothetical_ancestor_1 are from same hypothetical_ancestor_ 2 .\",\n",
        "  \"hypothetical_ancestor_2 and Tiger_Shark are from same hypothetical_ancestor_ 3 .\",\n",
        "  \"Gray_Reef_Shark and Hammerhead_Shark are from same hypothetical_ancestor_ 4 .\",\n",
        "  \"hypothetical_ancestor_3 and hypothetical_ancestor_4 are from same hypothetical_ancestor_ 5 .\"\n",
        "]\n",
        "\n",
        "\n",
        "questions = [\n",
        "  (\"Who is the parent of Mako_Shark?\",\"hypothetical_ancestor_ 1\"),\n",
        "  (\"Who is the parent of Great_White_Shark?\",\"hypothetical_ancestor_ 1\"),\n",
        "  (\"Who is the parent of Great_White_Shark and Marko_Shark?\",\"hypothetical_ancestor_ 1\"),\n",
        "  (\"Who is the parent of Gray_Reef_Shark?\",\"hypothetical_ancestor_ 4\"),\n",
        "  (\"Who is the parent of Hammerhead_Shark?\",\"hypothetical_ancestor_ 4\"),\n",
        "  (\"Who is the parent of Tiger_Shark?\",\"hypothetical_ancestor_2\"),\n",
        "  (\"Who is the parent of Angle_Shark?\",\"hypothetical_ancestor_ 2\"),\n",
        "  (\"Who is the parent of Gray_Reef_Shark and Hammerhead_Shark?\",\"same hypothetical_ancestor_ 4\"),\n",
        "\n",
        "  (\"Who are the children of hypothetical_ancestor_1?\",\"Great_White_Shark and Marko_Shark\"),\n",
        "  (\"Who are the children of hypothetical_ancestor_4?\",\"Gray_Reef_Shark and Hammerhead_Shark\"),\n",
        "  (\"Who are the children of hypothetical_ancestor_3?\",\"hypothetical_ancestor_2 and Tiger_Shark\"),\n",
        "  (\"Who are the children of hypothetical_ancestor_2?\",\"Angel_Shark and hypothetical_ancestor_1\"),\n",
        "\n",
        "  (\"What is the common ancestor of Marko_Shark and Great_White_Shark?\",\"hypothetical_ancestor_1\"),\n",
        "  (\"What is the common ancestor of Gray_Reef_Shark and Hammerhead_Shark?\",\"hypothetical_ancestor_4\"),\n",
        "  (\"What is the common ancestor of  Marko_Shark,Great_White_Shark and Angle_Shark?\",\"hypothetical_ancestor_2\"),\n",
        "  (\"What is the common ancestor of  Marko_Shark,Great_White_Shark,Angle_Shark and Tiger_Shark?\",\"hypothetical_ancestor_3\"),\n",
        "  (\"What is the common ancestor of  Marko_Shark,Great_White_Shark, Angle_Shark,Tiger_Shark,Gray_Reef_Shark and Hammerhead_Shark?\",\"hypothetical_ancestor_5\"),\n",
        "\n",
        "  (\"What is the relationship between Great_White_Shark and Tiger_Shark?\",\"same hypothetical_ancestor_ 3\"),\n",
        "  (\"What is the relationship between Great_White_Shark and Marko_Shark?\",\"same hypothetical_ancestor_ 1\")\n",
        "\n",
        "]\n",
        "\n",
        "# generate answers and calculate F1 score\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "for question in questions:\n",
        "    true_labels.append(question[1])\n",
        "    answer = generate_answer(question[0], ' '.join(sentences))\n",
        "    print(f\"Q: {question[0]}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    predicted_labels.append(answer.strip())\n",
        "\n",
        "\n",
        "y_true = true_labels\n",
        "y_pred = predicted_labels\n",
        "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
        "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "exact_match = 0\n",
        "n = len(y_true)\n",
        "for i in range(n):\n",
        "  if y_true[i]==y_pred[i]:\n",
        "    exact_match = exact_match + 1\n",
        "#exact_match = int(y_true == y_pred)\n",
        "\n",
        "# Print the output and metrics\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Exact Match:\", exact_match)"
      ],
      "metadata": {
        "id": "pOsbHwfSSacS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **T5_BioQA F1_Score**"
      ],
      "metadata": {
        "id": "X8LLgPHTUHaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load the tokenizer and model\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"ozcangundes/T5-base-for-BioQA\").to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"ozcangundes/T5-base-for-BioQA\")\n",
        "\n",
        "\n",
        "# define the function to generate answers\n",
        "def generate_answer(question, context):\n",
        "    input_str = \"question: \" + question + \" context: \" + context #+ \" </s>\"\n",
        "    input_ids = tokenizer.encode(input_str, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(input_ids)\n",
        "    # print(\"Output\")\n",
        "    # print(tokenizer.decode(outputs[0]))\n",
        "    tem = tokenizer.decode(outputs[0])\n",
        "    ans = \"\"\n",
        "    n = len(tem)\n",
        "    f = True\n",
        "    for c in range(0,n):\n",
        "      if tem[c]=='<':\n",
        "        f = False\n",
        "      elif tem[c]=='>':\n",
        "        f = True\n",
        "      else:\n",
        "        if f==True:\n",
        "          ans = ans + tem[c]\n",
        "    print(ans)\n",
        "    return ans\n",
        "    # return tokenizer.decode(outputs[0])\n",
        "\n",
        "# define the function to calculate F1 score\n",
        "def calculate_f1_score(true_labels, predicted_labels):\n",
        "    return f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# example sentences\n",
        "sentences = [\n",
        "  \"Mako_Shark and Great_White_Shark are from same hypothetical_ancestor_ 1 .\",\n",
        "  \"Angel_Shark and hypothetical_ancestor_1 are from same hypothetical_ancestor_ 2 .\",\n",
        "  \"hypothetical_ancestor_2 and Tiger_Shark are from same hypothetical_ancestor_ 3 .\",\n",
        "  \"Gray_Reef_Shark and Hammerhead_Shark are from same hypothetical_ancestor_ 4 .\",\n",
        "  \"hypothetical_ancestor_3 and hypothetical_ancestor_4 are from same hypothetical_ancestor_ 5 .\"\n",
        "]\n",
        "\n",
        "questions = [\n",
        "    (\"Who is the parent of Mako_Shark?\",\"hypothetical_ancestor_ 1\"),\n",
        "    (\"Who is the parent of Great_White_Shark?\",\"hypothetical_ancestor_ 1\"),\n",
        "    (\"Who is the parent of Great_White_Shark and Marko_Shark?\",\"same hypothetical_ancestor_ 1\"),\n",
        "    (\"Who is the parent of Gray_Reef_Shark?\",\"hypothetical_ancestor_ 4\"),\n",
        "    (\"Who is the parent of Hammerhead_Shark?\",\"hypothetical_ancestor_ 4\"),\n",
        "    (\"Who is the parent of Tiger_Shark?\",\"hypothetical_ancestor_3\"),\n",
        "    (\"Who is the parent of Angle_Shark?\",\"hypothetical_ancestor_ 2\"),\n",
        "    (\"Who is the parent of Gray_Reef_Shark and Hammerhead_Shark?\",\"same hypothetical_ancestor_ 4\"),\n",
        "\n",
        "    (\"Who are the children of hypothetical_ancestor_1?\",\"Great_White_Shark and Marko_Shark\"),\n",
        "    (\"Who are the children of hypothetical_ancestor_4?\",\"Gray_Reef_Shark and Hammerhead_Shark\"),\n",
        "    (\"Who are the children of hypothetical_ancestor_3?\",\"Tiger_Shark\"),\n",
        "    (\"Who are the children of hypothetical_ancestor_2?\",\"Angel_Shark and hypothetical_ancestor_1\"),\n",
        "\n",
        "    (\"What is the common ancestor of Marko_Shark and Great_White_Shark?\",\"hypothetical_ancestor_1\"),\n",
        "    (\"What is the common ancestor of Gray_Reef_Shark and Hammerhead_Shark?\",\"same hypothetical_ancestor_4\"),\n",
        "    (\"What is the common ancestor of  Marko_Shark,Great_White_Shark and Angle_Shark?\",\"hypothetical_ancestor_2\"),\n",
        "    (\"What is the common ancestor of  Marko_Shark,Great_White_Shark,Angle_Shark and Tiger_Shark?\",\"hypothetical_ancestor_3\"),\n",
        "    (\"What is the common ancestor of  Marko_Shark,Great_White_Shark, Angle_Shark,Tiger_Shark,Gray_Reef_Shark and Hammerhead_Shark?\",\"hypothetical_ancestor_5\"),\n",
        "\n",
        "    (\"What is the relationship between Great_White_Shark and Tiger_Shark?\",\"same hypothetical_ancestor_ 3\"),\n",
        "    (\"What is the relationship between Great_White_Shark and Marko_Shark?\",\"same hypothetical_ancestor_ 1\")\n",
        "]\n",
        "\n",
        "# generate answers and calculate F1 score\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "for question in questions:\n",
        "    true_labels.append(question[1])\n",
        "    answer = generate_answer(question[0], ' '.join(sentences))\n",
        "    print(f\"Q: {question[0]}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    predicted_labels.append(answer.strip())\n",
        "\n",
        "\n",
        "y_true = true_labels\n",
        "y_pred = predicted_labels\n",
        "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
        "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
        "#exact_match = int(y_true == y_pred)\n",
        "exact_match = 0\n",
        "n = len(y_true)\n",
        "for i in range(n):\n",
        "  if y_true[i]==y_pred[i]:\n",
        "    exact_match = exact_match + 1\n",
        "\n",
        "# Print the output and metrics\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Exact Match:\", exact_match)"
      ],
      "metadata": {
        "id": "hWj_eVXOUFf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ALBERT F1_Score**"
      ],
      "metadata": {
        "id": "9eOmA-WtVkoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load ALBERT model and tokenizer\n",
        "model_name = \"mfeb/albert-xxlarge-v2-squad2\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# define the function to generate answers\n",
        "def generate_answer(question, context):\n",
        "    nlp = pipeline('question-answering', model=model, tokenizer=tokenizer, device=device)\n",
        "    result = nlp(question=question, context=context)\n",
        "    answer = result['answer']\n",
        "    return answer\n",
        "\n",
        "# define the function to calculate F1 score\n",
        "def calculate_f1_score(true_labels, predicted_labels):\n",
        "    return f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# example sentences\n",
        "sentences = [\n",
        "  \"Mako_Shark and Great_White_Shark are from same hypothetical_ancestor_ 1 .\",\n",
        "  \"Angel_Shark and hypothetical_ancestor_1 are from same hypothetical_ancestor_ 2 .\",\n",
        "  \"hypothetical_ancestor_2 and Tiger_Shark are from same hypothetical_ancestor_ 3 .\",\n",
        "  \"Gray_Reef_Shark and Hammerhead_Shark are from same hypothetical_ancestor_ 4 .\",\n",
        "  \"hypothetical_ancestor_3 and hypothetical_ancestor_4 are from same hypothetical_ancestor_ 5 .\"\n",
        "]\n",
        "\n",
        "questions = [\n",
        "    (\"Who is the parent of Mako_Shark?\",\"hypothetical_ancestor_ 1\"),\n",
        "    (\"Who is the parent of Great_White_Shark?\",\"hypothetical_ancestor_1\"),\n",
        "    (\"Who is the parent of Great_White_Shark and Marko_Shark?\",\"hypothetical_ancestor_1\"),\n",
        "    (\"Who is the parent of Gray_Reef_Shark?\",\"hypothetical_ancestor_4\"),\n",
        "    (\"Who is the parent of Hammerhead_Shark?\",\"hypothetical_ancestor_ 4\"),\n",
        "    (\"Who is the parent of Tiger_Shark?\",\"hypothetical_ancestor_ 3\"),\n",
        "    (\"Who is the parent of Angle_Shark?\",\"hypothetical_ancestor_ 2\"),\n",
        "    (\"Who is the parent of Gray_Reef_Shark and Hammerhead_Shark?\",\"hypothetical_ancestor_ 4\"),\n",
        "\n",
        "    (\"Who are the children of hypothetical_ancestor_1?\",\"Great_White_Shark and Marko_Shark\"),\n",
        "    (\"Who are the children of hypothetical_ancestor_4?\",\"Gray_Reef_Shark and Hammerhead_Shark\"),\n",
        "    (\"Who are the children of hypothetical_ancestor_3?\",\"hypothetical_ancestor_2 and Tiger_Shark\"),\n",
        "    (\"Who are the children of hypothetical_ancestor_2?\",\"Angel_Shark and hypothetical_ancestor_1\"),\n",
        "\n",
        "    (\"What is the common ancestor of Marko_Shark and Great_White_Shark?\",\"hypothetical_ancestor_ 1\"),\n",
        "    (\"What is the common ancestor of Gray_Reef_Shark and Hammerhead_Shark?\",\"hypothetical_ancestor_ 4\"),\n",
        "    (\"What is the common ancestor of  Marko_Shark,Great_White_Shark and Angle_Shark?\",\"hypothetical_ancestor_ 2\"),\n",
        "    (\"What is the common ancestor of  Marko_Shark,Great_White_Shark,Angle_Shark and Tiger_Shark?\",\"hypothetical_ancestor_3\"),\n",
        "    (\"What is the common ancestor of  Marko_Shark,Great_White_Shark, Angle_Shark,Tiger_Shark,Gray_Reef_Shark and Hammerhead_Shark?\",\"hypothetical_ancestor_5\"),\n",
        "\n",
        "    (\"What is the relationship between Great_White_Shark and Tiger_Shark?\",\"Same hypthetical_ancestor_3\"),\n",
        "    (\"What is the relationship between Great_White_Shark and Marko_Shark?\",\"Sibling\")\n",
        "]\n",
        "\n",
        "# generate answers and calculate F1 score\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "for question in questions:\n",
        "    true_labels.append(question[1])\n",
        "    answer = generate_answer(question[0], ' '.join(sentences))\n",
        "    print(f\"Q: {question[0]}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    predicted_labels.append(answer.strip())\n",
        "\n",
        "y_true = true_labels\n",
        "y_pred = predicted_labels\n",
        "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
        "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
        "#exact_match = int(y_true == y_pred)\n",
        "exact_match = 0\n",
        "n = len(y_true)\n",
        "for i in range(n):\n",
        "  if y_true[i]==y_pred[i]:\n",
        "    exact_match = exact_match + 1\n",
        "\n",
        "# Print the output and metrics\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Exact Match:\", exact_match)\n"
      ],
      "metadata": {
        "id": "L4nHIbpgNrm6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}